{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "064c1d56",
   "metadata": {},
   "source": [
    "# Coding Block 4 - Afternoon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36824425",
   "metadata": {},
   "source": [
    "## Optimizing Decision Tree Performance\n",
    "\n",
    "What are criteria we want to optimize the code for?\n",
    "\n",
    "- **criterion :  optional (default=”gini”) or Choose attribute selection measure**: This parameter allows us to use the different-different attribute selection measure. Supported criteria are “gini” for the Gini index and “entropy” for the information gain.\n",
    "\n",
    "- **splitter : string, optional (default=”best”) or Split Strategy**: This parameter allows us to choose the split strategy. Supported strategies are “best” to choose the best split and “random” to choose the best random split.\n",
    "\n",
    "- **max_depth : int or None, optional (default=None) or Maximum Depth of a Tree**: The maximum depth of the tree. If None, then nodes are expanded until all the leaves contain less than min_samples_split samples. The higher value of maximum depth causes overfitting, and a lower value causes underfitting ([Source](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)).\n",
    "\n",
    "In Scikit-learn, optimization of decision tree classifier performed by only pre-pruning. Maximum depth of the tree can be used as a control variable for pre-pruning. In the following the example, you can plot a decision tree on the same data with max_depth=3.  Other than pre-pruning parameters, You can also try other attribute selection measure such as entropy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487c8ab6",
   "metadata": {},
   "source": [
    "### Load the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "385f3992",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n...\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from plotly.offline import init_notebook_mode,iplot\n",
    "from plotly.tools import FigureFactory as ff\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import *\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "'''\n",
    "...\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb60ee02",
   "metadata": {},
   "source": [
    "### Read the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea7f9e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "diab=pd.read_csv('C:\\\\Users\\\\v.weber\\\\Documents\\\\000 Master Wirtschaftsinformatik FU Berlin\\\\I\\\\Applied Analytics\\\\github stuff\\\\fork\\\\Applied-Analytics\\\\data\\\\diabetes.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f21ff5",
   "metadata": {},
   "source": [
    "## Do some hyperparameter tuning to benchmark different decision tree models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0803aa48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X = diab.drop(columns=['Outcome'])  # Feature variables\n",
    "y = diab['Outcome']  # Outcome variable\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4a21122d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters:\n",
      "{'max_depth': 4, 'min_samples_split': 2, 'min_samples_leaf': 45}\n",
      "Best F1-Score: 0.6750\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Define hyperparameter ranges\n",
    "max_depth_range = range(1, 10, 1)  # Values from 1 to 50 in steps of 1\n",
    "min_samples_split_range = range(2, 10, 1)  # Values from 2 to 50 in steps of 1\n",
    "min_samples_leaf_range = range(30, 51, 1)  # Values from 1 to 50 in steps of 1\n",
    "\n",
    "# Initialize variables to store the best model and score\n",
    "best_f1_score = 0\n",
    "best_hyperparameters = {}\n",
    "\n",
    "# Iterate over all combinations of hyperparameters\n",
    "for max_depth in max_depth_range:\n",
    "    for min_samples_split in min_samples_split_range:\n",
    "        for min_samples_leaf in min_samples_leaf_range:\n",
    "            # Build and train the decision tree model\n",
    "            model = DecisionTreeClassifier(\n",
    "                random_state=42,\n",
    "                max_depth=max_depth,\n",
    "                min_samples_split=min_samples_split,\n",
    "                min_samples_leaf=min_samples_leaf\n",
    "            )\n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "            # Evaluate the model on the test data\n",
    "            y_test_pred = model.predict(X_test)\n",
    "            f1 = f1_score(y_test, y_test_pred)\n",
    "            \n",
    "            # Update the best model if the current one is better\n",
    "            if f1 > best_f1_score:\n",
    "                best_f1_score = f1\n",
    "                best_hyperparameters = {\n",
    "                    'max_depth': max_depth,\n",
    "                    'min_samples_split': min_samples_split,\n",
    "                    'min_samples_leaf': min_samples_leaf\n",
    "                }\n",
    "\n",
    "# Print the best hyperparameters and corresponding F1-score\n",
    "print(\"Best Hyperparameters:\")\n",
    "print(best_hyperparameters)\n",
    "print(f\"Best F1-Score: {best_f1_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9af706c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Rules:\n",
      "|--- Glucose <= 143.50\n",
      "|   |--- Age <= 28.50\n",
      "|   |   |--- Glucose <= 126.50\n",
      "|   |   |   |--- BMI <= 31.40\n",
      "|   |   |   |   |--- SkinThickness <= 14.50\n",
      "|   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |--- SkinThickness >  14.50\n",
      "|   |   |   |   |   |--- class: 0\n",
      "|   |   |   |--- BMI >  31.40\n",
      "|   |   |   |   |--- Glucose <= 105.50\n",
      "|   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |--- Glucose >  105.50\n",
      "|   |   |   |   |   |--- class: 0\n",
      "|   |   |--- Glucose >  126.50\n",
      "|   |   |   |--- class: 0\n",
      "|   |--- Age >  28.50\n",
      "|   |   |--- BMI <= 26.95\n",
      "|   |   |   |--- class: 0\n",
      "|   |   |--- BMI >  26.95\n",
      "|   |   |   |--- Glucose <= 100.50\n",
      "|   |   |   |   |--- class: 0\n",
      "|   |   |   |--- Glucose >  100.50\n",
      "|   |   |   |   |--- DiabetesPedigreeFunction <= 0.50\n",
      "|   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |--- DiabetesPedigreeFunction >  0.50\n",
      "|   |   |   |   |   |--- class: 1\n",
      "|--- Glucose >  143.50\n",
      "|   |--- Glucose <= 166.50\n",
      "|   |   |--- class: 1\n",
      "|   |--- Glucose >  166.50\n",
      "|   |   |--- class: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import export_text\n",
    "\n",
    "# Export the decision rules as text\n",
    "decision_tree_rules = export_text(decision_tree_model, feature_names=list(X.columns))\n",
    "print(\"Decision Tree Rules:\")\n",
    "print(decision_tree_rules)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9891d2",
   "metadata": {},
   "source": [
    "## Use the information of the decision tree classifier to produce simple plots and information for stakeholders.\n",
    "What are some relevant patterns to predict diabetes?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdeccb95",
   "metadata": {},
   "source": [
    "High Glucose value (>143.5) && age > 29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850f5815",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0c48b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 49.933279,
   "end_time": "2021-09-20T20:03:41.801824",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-09-20T20:02:51.868545",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
